{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seance_1_trous.ipynb","provenance":[{"file_id":"12kzjH9ZgGgI13H1jf7FanxwRYbNQEJQ0","timestamp":1631303357169}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ydf0BUawr40O"},"source":["# ENS'IA - Séance 1 - Introduction au machine learning"]},{"cell_type":"code","metadata":{"id":"ujkuK3eUG--r"},"source":["import keras\n","from keras.datasets import cifar10\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import math\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvRbppLmtDPI"},"source":["Pour vous présenter les notions principales du Machine Learning, nous allons vous introduire deux algorithmes de base : KNN et K-MEAN.\n","\n","Ils seront appliqués au jeu de données CIFAR 10, jeu de données de 50 000 images appartenant à 10 classes d'images différentes."]},{"cell_type":"code","metadata":{"id":"tg-w9JxkHUFC"},"source":["# Charge le jeu de données\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# Observons les dimensions du jeu de données\n","print(\"Images du jeu d'entrainement {}\".format(x_train.shape));\n","print(\"Classes du jeu d'entrainement {}\".format(y_train.shape));\n","\n","# Classes des images de CIFAR-10\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCPJCZpvsXLB"},"source":["# Visualisons un exemple et sa classe\n","img_index = np.random.randint(0, x_train.shape[0])\n","plt.imshow(x_train[img_index])\n","plt.show()\n","\n","class_indx = y_train[img_index, 0]\n","print(\"Qui appartient à la classe {} ({})\".format(class_indx, classes[class_indx]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoPPlTlmx29Q"},"source":["# Grille d'exemples pour chaque classe\n","\n","num_classes = len(classes)\n","samples_per_class = 7\n","for y, cls in enumerate(classes):\n","    # Sélectionne aléatoirement des exemples de la classe y\n","    idxs = np.flatnonzero(y_train == y)\n","    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n","\n","    # Affiche ces exemples en colonne\n","    for i, idx in enumerate(idxs):\n","        plt_idx = i * num_classes + y + 1\n","        plt.subplot(samples_per_class, num_classes, plt_idx)\n","        plt.imshow(x_train[idx].astype('uint8'))\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06RJZmTwurDx"},"source":["K-NN (K Nearest Neighbor ou méthode des K plus proches voisins) est un algorithme consistant à trouver, dans le jeu de données d'entrainement, les K images ressemblant le plus à l'image dont nous souhaitons trouver la classe. Pour calculer la ressemblance entre deux images on peut en première approximation considérer simplement leur distance euclidienne (norme L2). Sur les K images trouvées, nous regardons ensuite quelle classe est la plus présente. On pourra ainsi décider de la classe de notre image de test.\n"]},{"cell_type":"code","metadata":{"id":"NVJ9wAKP-tA3"},"source":["# Redimensionne les images en les applatissant afin de faciliter\n","# leur manipulation\n","# On veut les shapes suivantes:\n","\n","# x_train: (50000, 32 * 32 * 3)\n","# x_test: (10000, 32 * 32 * 3)\n","# y_train: (Nombre de samples d'entraînement, )\n","# y_test:  (Nombre de samples de test, )\n","x_train = \n","x_test = \n","y_train = \n","y_test = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8adznnRX9Lv"},"source":["# Valeur du paramètre\n","k = 20"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHmodlF5yfPs"},"source":["De plus, utiliser les 50 000 images d'entraînement pour classer les 10 000 images de test serait *long*. Nous allons donc sélectionner une partie des deux ensembles:"]},{"cell_type":"code","metadata":{"id":"G_PZ29rmyzqt"},"source":["nb_imgs_train = 5000\n","nb_imgs_test = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bxx7vRIk24SK"},"source":["# --- Méthode brute ---\n","predictions = np.empty((nb_imgs_test, ))\n","for id_img_test, img_test in enumerate(x_test[:nb_imgs_test]):\n","  # Le tableau k_nearest contient les classes des k images les plus proches\n","  # distances contient les distances entre l'image test et les k plus proches\n","  k_nearest, distances = np.full((k, ), -1), np.full((k, ), float(\"+inf\"))\n","\n","  # On cherche à remplir le tableau k_nearest avec les classes des k\n","  # images d'entraînement les \"plus proches\" au sens de la distance euclidienne.\n","  # N'hésitez pas à chercher comment calculer la norme d'une image numpy :) \n","  for id_img_train, img_train in enumerate(x_train[:nb_imgs_train]):\n","    # TODO\n","    pass\n","\n","  # Compte le nombre d'items de chaque classe dans k_nearest et met dans\n","  # predictions celle qui est la plus présente\n","  predictions[id_img_test] = np.argmax(np.bincount(k_nearest))\n","  \n","  print(\"Classified image {}/{} \".format(id_img_test + 1, nb_imgs_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lTApXkk7reYd"},"source":["Cependant le code au dessus est pas très beau et le réimplémenter est un peu pénible.\n","Afin de simplifier la vie de tout le monde, nous allons utiliser une bibliothèque du nom de **sickit learn**.\n","Cette bibliothèque est une boite à outils remplie de beaucoup de fonctions très pratiques et d'algorithmes d'apprentissages prêts à l'utilisation. On vous laisse chercher [ici](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) comment l'utiliser. \n","**Nous vous déconseillons de faire l'entrainement sur les 50 000 éléments de x_train et le test sur les 10 000 éléments de x_test car cela vous prendrait trop de temps pour tester...**"]},{"cell_type":"code","metadata":{"id":"9UJcNPuu2ND4"},"source":["# Ne prenez pas trop d'images !\n","nb_imgs_train = 2000\n","nb_imgs_test = 500"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtGhy62K5PhP"},"source":["x_test = x_test[:nb_imgs_test]\n","y_test = y_test[:nb_imgs_test]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfCuTLT29v-r"},"source":["# On importe la bibliothèque\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# Regardez la fonction fit et la fonction score...\n","# On crée un modèle de paramètre k=7\n","# TODO !\n","\n"," # On entraine notre modèle\n","neigh.fit(x_train[:nb_imgs_train], y_train[:nb_imgs_train])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQUSobIfGgEL"},"source":["# Prédit les classes de x_test et calcule son score\n","print(neigh.score(x_test,y_test)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B95S1weT1e9M"},"source":["Affichons quelques exemples de classes attribuées par le KNN:"]},{"cell_type":"code","metadata":{"id":"XUB2Z7002_63"},"source":["from itertools import product"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ZJ73wNy1kf3"},"source":["# On affiche une grille carrée d'images\n","nb_cols = 4\n","fig, axes = plt.subplots(nrows=nb_cols, ncols=nb_cols, figsize=(8, 8))\n","samples = x_test[:nb_cols ** 2]\n","predictions = neigh.predict(samples)\n","\n","# On remet les exemples sous la forme d'images\n","samples = samples.reshape(samples.shape[0], 32, 32, 3)\n","\n","for i, j in product(range(nb_cols), range(nb_cols)):\n","    axes[i, j].imshow(samples[i * nb_cols + j])\n","    axes[i, j].axis(\"off\")\n","    axes[i, j].set_title(classes[predictions[i * nb_cols + j]])\n","\n","fig.suptitle(\"Quelques prédictions...\")\n","plt.show(fig)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7vJQRnJzi4z"},"source":["La fonction score nous a permis de mesure la précision (accuracy) de l'algorithme KNN sur une partie de notre\n","jeu de données.\n","Nous avons obtenu 0.29 ce qui veut dire que sur les 100 images testées, seules 29% étaient correctes.\n","De plus, vous avez pu remarquer que le temps d'exécution était plutôt long. Imaginez le temps que cela mettrait si l'on voulait tester l'intégralité de notre jeu de test\n","qui avait 10 000 exemples....\n","Ici, nous avons testé pour k = 7. Mais quelle est la valeur optimale de $k$ ? $k$ est ce que l'on apelle un **hyperparamètre**. C'est une valeur à configurer avant l'entrainement de notre modèle sur le jeu de données.\n","A vous de la trouver...\n"]},{"cell_type":"code","metadata":{"id":"SuF31W-_xNyq"},"source":["resultats = []\n","for k in range(1, 16):\n","    # TOUDOU:\n","    # Entraîner un modèle KNN de paramètre k\n","    # Tester ses performances sur x_test\n","    # Enregistrer le score dans resultats\n","    pass\n","\n","plt.plot(list(range(1, 16)), resultats, \"-+\")\n","plt.xlabel(\"K\")\n","plt.ylabel(\"Accuracy\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5fWSx-XOXBLa"},"source":["La méthode K-MEAN (ou méthode des K-moyennes) est un algorithme de partionnement de données (*clustering* en anglais). C'est l'un des algorithmes les plus fondamentaux en apprentissage non supervisé. L'algorithme consiste  à partionner des données pour tenter d'en dégager des classes. Dans notre cas, appliqué aux images de CIFAR-10, cela  revient a classer les images tel que cela est déjà fait mais en utilisant juste les données brutes (c'est pour cela que l'on parle d'apprentissage *non supervisé*). L'idée générale derrière la méthode est de regrouper les données en fonction de leurs ressemblances, i.e. de leur distance. L'algorithme fonctionne ainsi: on commence en considérant K données aléatoires, elles sont chacunes représentantes d'une classe ; à chaque itération, on va partionner les données en fonction de la ressemblance avec les K images types de départ : on regroupe dans la classe K toutes les images étant plus proches de la K-ème image type ; on calcule ensuite la moyenne des classes obtenues et l'on remplace l'image type de chacune des classes obtenues par cette moyenne.\n","\n","Ci-dessous nous avons tracé"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"KSnx0-N9XCM3"},"source":["K_VALUE = 10\n","\n","min_val = 1\n","# On initialise les K representants de chaque classe\n","K_mean = [255 * np.random.rand(32*32*3) for _ in range(10)]\n","# Valeur précédente de ces representants\n","K_save = [255 * np.random.rand(32*32*3) for _ in range(10)]\n","\n","def nearest_K(image):\n","  \"\"\"\n","  Retourne la classe K la plus proche de image\n","  \"\"\"\n","  min_dist, min_k = float(\"+inf\"), None\n","  for id_K, K_point in enumerate(K_mean):\n","    dist = np.linalg.norm(image - K_point)\n","    if dist < min_dist:\n","      min_dist, min_k = dist, id_K\n","  return min_k\n","\n","def mean_point(k, tab):\n","  \"\"\"\n","  Retourne barycentre des points (indicés) de tab\n","  \"\"\"\n","  if tab != []:\n","    mean = 0\n","    for id in tab:\n","      mean += x_train[id] / len(tab)\n","    K_mean[k] = mean\n","    \n","def stop_convergence():\n","  \"\"\"\n","  Evalue si l'on arrete les itérations\n","  \"\"\"\n","  for k in range(10):\n","    if not(np.array_equal(K_mean[k], K_save[k])):\n","      return True\n","  return False\n","\n","#KMEAN\n","iteration = 0\n","while stop_convergence():\n","  iteration += 1\n","  K_nearest = [[] for _ in range(10)]\n","\n","  for id_image, image in enumerate(x_train):\n","    K_nearest[nearest_K(image)].append(id_image)\n","\n","  for k in range(10):\n","    K_save[k] = K_mean[k]\n","    mean_point(k, K_nearest[k])\n","  print(iteration) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hzKvM7s-su4"},"source":["Essayons avec une fonction built-in écrite par de vrais Data Scientists:"]},{"cell_type":"code","metadata":{"id":"11F-doR7-q_9"},"source":[" from sklearn.cluster import KMeans\n","kmeans = KMeans(n_clusters=10);\n","kmeans.fit(x_train);\n","kmeans.score(x_test,y_test)"],"execution_count":null,"outputs":[]}]}