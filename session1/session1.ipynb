{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ydf0BUawr40O"
   },
   "source": [
    "# Ens'IA - Session 1 - Introduction to machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujkuK3eUG--r"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvRbppLmtDPI"
   },
   "source": [
    "To introduce you to the main notions of Machine Learning, we will present two basic algorithms: KNN and K-MEANS.\n",
    "\n",
    "They will be applied to the CIFAR 10 dataset, a dataset of 50,000 images belonging to 10 different image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tg-w9JxkHUFC"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Let's observe the dimensions of the dataset\n",
    "print(\"Shape of the training samples: {}\".format(x_train.shape));\n",
    "print(\"Shape of the training targets: {}\".format(y_train.shape));\n",
    "\n",
    "# CIFAR-10 image classes\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCPJCZpvsXLB"
   },
   "outputs": [],
   "source": [
    "# Let's visualize an example and its class\n",
    "img_index = np.random.randint(0, x_train.shape[0])\n",
    "plt.imshow(x_train[img_index])\n",
    "plt.show()\n",
    "\n",
    "class_indx = y_train[img_index, 0]\n",
    "print(\"-> class {} ({})\".format(class_indx, classes[class_indx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoPPlTlmx29Q"
   },
   "outputs": [],
   "source": [
    "# Grid of examples of each class\n",
    "\n",
    "n_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    # Randomly select class y samples\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "\n",
    "    # Display theses examples in columns\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * n_classes + y + 1\n",
    "        plt.subplot(samples_per_class, n_classes, plt_idx)\n",
    "        plt.imshow(x_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06RJZmTwurDx"
   },
   "source": [
    "K-NN (K Nearest Neighbor) is an algorithm that consists in finding in the training dataset the K images that resembles the most the image we want to classify.\n",
    "\n",
    "To compute the resemblance between two images we can, as a first approximation, simply consider their Euclidean distance (L2 norm). On the K images found, we then look at which class is the most present: we can thus decide the class of our test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVJ9wAKP-tA3"
   },
   "outputs": [],
   "source": [
    "# Resize the images by flattening them to facilitate their manipulation\n",
    "# We want the following shapes:\n",
    "\n",
    "# x_train: (50000, 32 * 32 * 3)\n",
    "# x_test: (10000, 32 * 32 * 3)\n",
    "# y_train: (training sample count, )\n",
    "# y_test:  (testing sample count, )\n",
    "x_train = \n",
    "x_test = \n",
    "y_train = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8adznnRX9Lv"
   },
   "outputs": [],
   "source": [
    "# Parameter value\n",
    "k = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHmodlF5yfPs"
   },
   "source": [
    "Using all 50 000 training images to classify the 10 000 test images would take .. well .. *some time*. Therefore we're going to use subsets of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_PZ29rmyzqt"
   },
   "outputs": [],
   "source": [
    "n_imgs_train = 5000\n",
    "n_imgs_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bxx7vRIk24SK"
   },
   "outputs": [],
   "source": [
    "# --- Bruteforce method ---\n",
    "predictions = np.empty((n_imgs_test, ))\n",
    "for id_img_test, img_test in enumerate(x_test[:n_imgs_test]):\n",
    "  # \"k_nearest\" array contains the classes of the k nearest images.\n",
    "  # \"distances\" array contains the distances between the test image and the k nearest\n",
    "  k_nearest, distances = np.full((k, ), -1), np.full((k, ), float(\"+inf\"))\n",
    "\n",
    "  # We want to fill k_nearest with the classes of the k training images that\n",
    "  # are closest to img_test (w.r. to the euclidean distance).\n",
    "  # Feel free to check out how to compute the euclidean norm of an image\n",
    "  # in the NumPy documentation :)\n",
    "  for id_img_train, img_train in enumerate(x_train[:n_imgs_train]):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "  # Counts the number of items of each class in k_nearest, and puts in\n",
    "  # predictions that which appears most\n",
    "  predictions[id_img_test] = np.argmax(np.bincount(k_nearest))\n",
    "  \n",
    "  print(\"Classified image {}/{} \".format(id_img_test + 1, n_imgs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTApXkk7reYd"
   },
   "source": [
    "However this code is quite painful to write and read. To simplify and make it\n",
    "run faster, we'll use the famous package **scikit-learn**.\n",
    "This package really is *the* toolbox for ML developpers. It includes a lot of ready-to-use learning algorithms. We'll let you read about the KNN implentation by yourself [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n",
    "**Once more, we strongly advise against using all samples from the training and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UJcNPuu2ND4"
   },
   "outputs": [],
   "source": [
    "n_imgs_train = 2000\n",
    "n_imgs_test = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtGhy62K5PhP"
   },
   "outputs": [],
   "source": [
    "x_test = x_test[:n_imgs_test]\n",
    "y_test = y_test[:n_imgs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfCuTLT29v-r"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN model of parameter k = 7\n",
    "# TODO !\n",
    "\n",
    " # Trains the model\n",
    " # Ready up about the fit() and score() methods !\n",
    "neigh.fit(x_train[:n_imgs_train], y_train[:n_imgs_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQUSobIfGgEL"
   },
   "outputs": [],
   "source": [
    "# Makes the predictions for the test set and evaluates the accuracy\n",
    "print(neigh.score(x_test,y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B95S1weT1e9M"
   },
   "source": [
    "Let's visualize a few examples of predicted classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUB2Z7002_63"
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZJ73wNy1kf3"
   },
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "fig, axes = plt.subplots(nrows=n_cols, ncols=n_cols, figsize=(8, 8))\n",
    "samples = x_test[:n_cols ** 2]\n",
    "predictions = neigh.predict(samples)\n",
    "\n",
    "# Reshapes the samples into the image shape\n",
    "samples = samples.reshape(samples.shape[0], 32, 32, 3)\n",
    "\n",
    "for i, j in product(range(n_cols), range(n_cols)):\n",
    "    axes[i, j].imshow(samples[i * n_cols + j])\n",
    "    axes[i, j].axis(\"off\")\n",
    "    axes[i, j].set_title(classes[predictions[i * n_cols + j]])\n",
    "\n",
    "fig.suptitle(\"A few predictions...\")\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7vJQRnJzi4z"
   },
   "source": [
    "We obtained a score of 0.29 (this may vary with the amount of training and test samples you used). Moreover you've likely noticed that the algorithm requires quite some time (imagine using the whole datasets !).\n",
    "We've evaluated the model for $k= 7$, but what's the optimal value for $k$ ?\n",
    "$k$ is called an **hyperparameter**: it's a value on which the algorithm depends but which is *not* learned.\n",
    "We'll let you estimate the best value for $k$ on your own:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuF31W-_xNyq"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for k in range(1, 16):\n",
    "    # TODO:\n",
    "    # Train a KNN model of parameter k\n",
    "    # Evaluate its performance on a subset of x_test\n",
    "    # Save the accuracy in results\n",
    "    pass\n",
    "\n",
    "plt.plot(list(range(1, 16)), results, \"-+\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fWSx-XOXBLa"
   },
   "source": [
    "The K-Means method is a **data clustering** algorithm, among the fundamentals of *unsupervised learning*. This means that the algorithm **does not require any target data**.\n",
    "\n",
    "The algorithm tries to split the samples into separate groups, which can then be interpreted as classes. It starts by taking $K$ random images. Each of those original images is the *representative* of its class. Then for any training image $I$ we find the *representative* $R$ that is closest to $I$, and place $I$ in the class represented by $R$.\n",
    "After all training images have been placed in a class, we replace the representatives by the **mean** of their classes.\n",
    "We repeat this process until a convergence criterion is reached (for example, that the representatives have barely evolved between two successive steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KSnx0-N9XCM3"
   },
   "outputs": [],
   "source": [
    "K_VALUE = 10\n",
    "\n",
    "min_val = 1\n",
    "# Initializes the K representatives\n",
    "K_mean = [255 * np.random.rand(32*32*3) for _ in range(K_VALUE)]\n",
    "# Precedent values of the representatives\n",
    "K_save = [255 * np.random.rand(32*32*3) for _ in range(K_VALUE)]\n",
    "\n",
    "def nearest_K(image):\n",
    "  \"\"\"\n",
    "  Returns the class whose representative is closest to the image.\n",
    "  \"\"\"\n",
    "  min_dist, min_k = float(\"+inf\"), None\n",
    "  for id_K, K_point in enumerate(K_mean):\n",
    "    dist = np.linalg.norm(image - K_point)\n",
    "    if dist < min_dist:\n",
    "      min_dist, min_k = dist, id_K\n",
    "  return min_k\n",
    "\n",
    "def mean_point(k, tab):\n",
    "  \"\"\"\n",
    "  Replaces the representative of the k_th class with the mean\n",
    "  of tab. tab should be a list of images.\n",
    "  \"\"\"\n",
    "  if tab != []:\n",
    "    K_mean[k] = np.mean(tab, axis=0)\n",
    "    \n",
    "def stop_convergence():\n",
    "  \"\"\"\n",
    "  Evaluates whether we should stop iterating.\n",
    "  \"\"\"\n",
    "  for k in range(10):\n",
    "    # We might want a less strict criterion !\n",
    "    if np.array_equal(K_mean[k], K_save[k]):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "#KMEAN\n",
    "iteration = 0\n",
    "while not stop_convergence():\n",
    "  iteration += 1\n",
    "  K_nearest = [[] for _ in range(K_VALUE)]\n",
    "\n",
    "  for id_image, image in enumerate(x_train):\n",
    "    K_nearest[nearest_K(image)].append(id_image)\n",
    "\n",
    "  for k in range(K_VALUE):\n",
    "    K_save[k] = K_mean[k]\n",
    "    mean_point(k, K_nearest[k])\n",
    "  print(iteration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hzKvM7s-su4"
   },
   "source": [
    "Let's try with a built-in function written by some serious Data Scientists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11F-doR7-q_9"
   },
   "outputs": [],
   "source": [
    " from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10);\n",
    "kmeans.fit(x_train);\n",
    "kmeans.score(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SÃ©ance 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
